

<!doctype html>
<html>
   <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="chrome=1">
      <title>tDBN | optimal tree-augmented DBN learning</title>
      <link rel="stylesheet" href="stylesheets/styles.css">
      <link rel="stylesheet" href="stylesheets/pygment_trac.css">
      <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
      <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <![endif]-->
   </head>
   <body>
      <div class="wrapper">
      <header>
         <h1>tDBN</h1>
         <p>A Java implementation for learning optimal tree-augmented dynamic Bayesian
            networks
         </p>
         <p>Released under the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></p>
         <p class="view"><a href="https://github.com/josemonteiro/tDBN">View the Project on GitHub <small>tDBN/tDBN</small></a></p>
         <ul>
            <li><a href="https://github.com/josemonteiro/tDBN/releases/download/0.1.4/tDBN-0.1.4.jar">Download release <strong>tDBN-0.1.4</strong></a></li>
            <li><a href="https://github.com/josemonteiro/tDBN/zipball/master">Download current <strong>sources</strong></a></li>
         </ul>
      </header>
      <section>
         <h3>Program description</h3>
         <p>
            tDBN is Java implementation of a dynamic Bayesian network (DBN) structure learning algorithm with the same name (for details about the algorithm, please <a href="#algorithm">scroll down</a>). It can learn a network structure from a file with multivariate longitudinal observations, and has polynomial time complexity in the number of attributes and observations.
         </p>
         <p>
           If you use tDBN in your research, please cite:
         </p>
        <p>
           José L Monteiro, Susana Vinga, and Alexandra M Carvalho.<br/>
           Polynomial-time algorithm for learning optimal tree-augmented dynamic Bayesian networks.<br/>
           In UAI, pages 622–631, 2015. <a href="http://auai.org/uai2015/proceedings/papers/329.pdf">http://auai.org/uai2015/proceedings/papers/329.pdf</a>
        </p>
         <h4>Current release</h4>
         <p>
            The program's current release is tDBN-0.1.4, and can be downloaded <a href="https://github.com/josemonteiro/tDBN/releases/download/0.1.4/tDBN-0.1.4.jar">here</a>. It comes packaged as an executable JAR file, already including the required external libraries.
         </p>
         <h4>Libraries</h4>
         <p>
            tDBN depends on two external libraries, both released under the Apache License 2.0:
<ul><li><a href="http://opencsv.sourceforge.net/">opencsv</a> for parsing CSV files;
<li><a href="http://commons.apache.org/proper/commons-cli/">Apache Commons CLI</a> for parsing command line
options passed to the program, and also printing help messages detailing the
available options.
</ul>
         </p>

         <h3>Usage</h3>
         <p>By executing the jar file ...
         <pre><code>$ java -jar tDBN-0.1.3.jar</code></pre>
         ... the available command-line options are shown:
         </p>
         <pre><code>usage: tDBN
 -c,--compact                 Outputs network in compact format, omitting
                              intra-slice edges. Only works if specified
                              together with -d and with --markovLag 1.
 -d,--dotFormat               Outputs network in dot format, allowing
                              direct redirection into Graphviz to
                              visualize the graph.
 -i,--inputFile &lt;file&gt;        Input CSV file to be used for network
                              learning.
 -m,--markovLag &lt;int&gt;         Maximum Markov lag to be considered, which
                              is the longest distance between connected
                              time-slices. Default is 1, allowing edges
                              from one preceding slice.
 -ns,--nonStationary          Learns a non-stationary network (one
                              transition network per time transition). By
                              default, a stationary DBN is learnt.
 -o,--outputFile &lt;file&gt;       Writes output to &lt;file&gt;. If not supplied,
                              output is written to terminal.
 -p,--numParents &lt;int&gt;        Maximum number of parents from preceding
                              time-slice(s).
 -pm,--parameters             Learns and outputs the network parameters.
 -r,--root &lt;int&gt;              Root node of the intra-slice tree. By
                              default, root is arbitrary.
 -s,--scoringFunction &lt;arg&gt;   Scoring function to be used, either MDL or
                              LL. MDL is used by default.
 -sp,--spanning               Forces intra-slice connectivity to be a tree
                              instead of a forest, eventually producing a
                              structure with a lower score.
</code></pre>
         <h4>Input file format</h4>
         <p>
            The input file should be in comma-separated values (CSV) format.
         <ul>
            <li>The first line is the header, naming the attributes and specifying the time slice index, separared by two underscores: "attributeName__t"</li>
            <li>The order of the attributes must be maintained: "a__1", "b__1", "a__2", "b__2".
            <li>The first column contains an identification (string or number) of each subject (this identifier does not affect the learnt network).</li>
            <li>All other lines correspond to observations of an individual over time.</li>
            <li>Missing values can be marked as "?" but should not occur, as the algorithm discards the observation (time slice) in question.</li>
         </ul>
         A very simplistic input file example is the following:
         </p>
         <pre><code>subject_id,resp__1,age__1,height__1,stunt__1,resp__2,age__2,height__2,stunt__2
121013, 0, 67, -3, 0, 0, 70, -3, 0
121113, 0, 27,  2, 0, 0, 30,  0, 0
121114, 0, 10,  8, 0, 0, 13,  5, 0
121140, 0, 17,  5, 0, ?,  ?,  ?, ?
(...)
</code></pre>
         <p>
            Because the algorithm assumes a multinomial distribution, data should be already discretized to guarantee a manageable (i.e., small) number of states for each attribute, taking into account the number of observations. For example, if an attribute is observed 100 times throughout time and 50 different values are recorded, this will generally not provide enough information for learning accurate data dependences.
         </p>

<h4>Dataset preprocessing script</h3>

<p>A Python script for coverting panel data to the tDBN input format can be downloaded <a href="https://github.com/josemonteiro/tDBN/raw/master/vtoh.py">here</a>. Its description and available command-line options are the following:</p>

<pre><code>usage: vtoh.py [-h] [--timeInterval TIMEINTERVAL TIMEINTERVAL]
               [--ignoreColumns IGNORECOLUMNS [IGNORECOLUMNS ...]]
               [--discretizeColumns DISCRETIZECOLUMNS [DISCRETIZECOLUMNS ...]]
               [--numberBins NUMBERBINS] [--equalFrequencyBinning]
               filename idColumn timeColumn

Converts a comma-separated-values (csv) temporal data file in 'vertical
format' (panel data format, one observation per line) to an 'horizontal
format' (one subject per line) csv file. The data file must have a column with
the identification of the subject, as well as a column with the identification
of the time instant. Both of these columns are provided as arguments and must
be integer numbers. The time interval to be considered can be specified as an
argument. It is also possible to specify columns to be ignored and columns to
be discretized (as well as the number of bins and the discretization method).

positional arguments:
  filename              Input CSV file.
  idColumn              Column containing subject IDs.
  timeColumn            Column containing time entries.

optional arguments:
  -h, --help            show this help message and exit
  --timeInterval TIMEINTERVAL TIMEINTERVAL
                        Time interval limits to be considered in the output.
  --ignoreColumns IGNORECOLUMNS [IGNORECOLUMNS ...]
                        Columns not to be included in output file.
  --discretizeColumns DISCRETIZECOLUMNS [DISCRETIZECOLUMNS ...]
                        Columns to be discretized.
  --numberBins NUMBERBINS
                        Number of bins to use when discretizing, default 10.
  --equalFrequencyBinning
                        If specified, discretization is done using equal
                        frequency or quantiles discretization (default is
                        equal width discretization).
</code></pre>

         <h4>Example #1</h4>
         <p>
            The first example considers a synthetic network structure with 5 attributes, each taking 8 states and with one parent from the preceding slice ([t] denotes the time-slice):
         </p>
         <p><img src="images/example1-original.png">
         </p>
         <p>
            The above network that was sampled to produce the following observations files:
         <ul>
            <li><a href="files/synth-N50.csv">synth-N50.csv</a>, with 50 observed time transitions</li>
            <li><a href="files/synth-N150.csv">synth-N150.csv</a>, with 150 observed time transitions</li>
            <li><a href="files/synth-N250.csv">synth-N250.csv</a>, with 250 observed time transitions</li>
         </ul>
         As all nodes have exactly one parent from the past, the best options are to limit the number of parents with <code>-p 1</code> and use the log-likelihood (LL) score with <code>-s ll</code> to ensure that a maximum number of dependences is retrieved.</p>
         <p>The command to learn the network with the 50 observations file is
         </p>
         <pre><code>$ java -jar tDBN-0.1.3.jar -i synth-N50.csv -p 1 -s ll</code></pre>
         <p>
        and produces the following output:
         </p>
         <pre><code>Evaluating network with LL score.
Number of networks with max score: 2
Finding a maximum branching.
Network score: 202.75732463763217

-----------------

X1[0] -> X0[1]
X2[0] -> X1[1]
X3[0] -> X2[1]
X2[0] -> X3[1]
X3[0] -> X4[1]

X0[1] -> X1[1]
X3[1] -> X2[1]
X0[1] -> X3[1]
X3[1] -> X4[1]
</code></pre>
         <p>
            Activating the <code>-d</code> switch to directly output in dot format, and redirecting into <a href="http://www.graphviz.org/">Graphviz</a>...
         <pre><code>$ java -jar tDBN-0.1.3.jar -i synth-N50.csv -p 1 -s ll -d | dot -Tpng -o N50.png</code></pre>
         <p>...produces this graph:</p>
         <p><img src="images/example1-N50.png"></p>
         <p>
            Although there are some matches (X2[0]->X3[1] and X3[0]->X4[1]), the learnt network is very different from the original, because the number of observations is low.
         </p>
         <p>
            Using the 250 observations file as input to tDBN ...
         <pre><code>$ java -jar tDBN-0.1.3.jar -i synth-N250.csv -p 1 -s ll</code></pre>
         ... results in the following output, which is a perfect match of the original network:
         </p>
         <pre><code>Evaluating network with LL score.
Number of networks with max score: 1
Finding a maximum branching.
Network score: 829.3579534531533

-----------------

X0[0] -> X0[1]
X1[0] -> X1[1]
X4[0] -> X2[1]
X2[0] -> X3[1]
X3[0] -> X4[1]

X1[1] -> X0[1]
X1[1] -> X2[1]
X2[1] -> X3[1]
X2[1] -> X4[1]
</code></pre>
         <p>An improved visual representation shows the evolution of the learnt networks with the number of observations N (dashed edges are incorrect):
         </p>
         <center>
         <table>
         <tr>
         <td><center><img src="images/N50.png"></center></td>
         <td><center><img src="images/N150.png"></center></td>
         <td><center><img src="images/N250.png"></center></td>
         </tr>
         <tr>
         <td><center>N=50</center></td>
         <td><center>N=150</center></td>
         <td><center>N=250</center></td>
         </table>
         </center>
         <h4>Example #2</h4>
         <p>
         In the second example, tDBN is employed to learn a gene regulatory network. Specifically, it uses gene expression data related to the embryonic stage of Drosophila melanogaster [<a href="#ref1">1</a>]. The dataset was preprocessed as described in [<a href="#ref2">2</a>] and can be downloaded <a href="files/drosophila-embryonic.csv">here</a>. </p>

         <p>As intra-slice dependences in regulatory networks are usually not reported, the option <code>-c</code> omits them by outputting the network in compact form, where each edge represents a dependence between a node at time slice t+1 and its parent at the previous time slice t.</p>

         <p>For this example, each node will have at most two parents from the previous slice, which is enforced with <code>-p 2</code>. The minimum description length (MDL) criterion further limits the number of parents by preferring simpler network structures. The <code>-s mdl</code> option is currently redundant, as tDBN uses MDL by default.</p>

         <p>The learning command is thus</p>
         <pre><code>$ java -jar tDBN-0.1.3.jar -i drosophila-embryonic.csv -p 2 -d -c | \
  dot -Tpng -o drosophila.png</code></pre>

        <p>and the resulting network is:</p>
         <p><img src="images/example2-drosophila-e.png"></p>
<p>Six out of the eight retrieved regulations (excluding the self-loops) are also reported in [<a href="#ref3">3</a>], indicating a potentially good result of tDBN. It should be noted, however, that data preprocessing may have not been the same, and that the number of observations is quite low, thus leading to many networks with maximum score.


            <h4>Example #3</h4>

            <p>
            The last example shows how to output the network parameters and how to learn a non-stationary DBN. The dataset is very simple, consisting of 2 binary attributes measured in 7 individuals over 3 time-slices, and can be downloaded <a href="files/example3.csv">here</a>:
         </p>
         <pre><code>id,a__0,b__0,a__1,b__1,a__2,b__2
1,0,0,0,1,1,1
2,0,1,1,0,1,0
3,0,1,0,0,1,0
4,1,1,1,1,1,0
5,1,1,0,1,1,0
6,1,1,0,1,0,0
7,0,0,0,0,0,0
</code></pre>

<p>To learn both the structure and the parameters of a stationary network, the following command is used:</p>
<pre><code>$ java -jar tDBN-0.1.3.jar -i example3.csv -p 1 -s ll --parameters</code></pre>
<p>resulting in</p>
<pre><code>Evaluating network with LL score.
Number of networks with max score: 1
Finding a maximum branching.
Network score: 1.4715189255145624

-----------------

a[0] -> a[1]
a[0] -> b[1]

b[1] -> a[1]


a: [0.0, 1.0]
[a[0]=1.0, b[1]=0.0]: 0.500 0.500
[a[0]=0.0, b[1]=1.0]: 1.000 0.000
[a[0]=1.0, b[1]=1.0]: 0.667 0.333
[a[0]=0.0, b[1]=0.0]: 0.667 0.333

b: [0.0, 1.0]
[a[0]=0.0]: 0.750 0.250
[a[0]=1.0]: 0.000 1.000
</code></pre>

<p>For every attribute (in this case, "a" and "b"), its conditional probability distribution table is present in the output. For a given table, the probability of each state of the corresponding attribute is specified, conditioned on the configurations of its parents (one per line).</p>

<p>Proceeding to learn a non-stationary network...</p>
<pre><code>$ java -jar tDBN-0.1.3.jar -i example3.csv -p 1 -s ll --parameters --nonStationary</code></pre>
<p>... the output is:</p>
<pre><code>Evaluating network with LL score.
Number of networks with max score: 1
Number of networks with max score: 2
Finding a maximum branching.
Network score: 0.3397980735907944
Network score: 0.5924696128065006

-----------------

b[0] -> a[1]
a[0] -> b[1]

a[1] -> b[1]


a: [0.0, 1.0]
[b[0]=1.0]: 0.600 0.400
[b[0]=0.0]: 1.000 0.000

b: [0.0, 1.0]
[a[0]=1.0, a[1]=0.0]: 0.000 1.000
[a[0]=1.0, a[1]=1.0]: 0.000 1.000
[a[0]=0.0, a[1]=1.0]: 1.000 0.000
[a[0]=0.0, a[1]=0.0]: 0.667 0.333

-----------------

a[1] -> a[2]
b[1] -> b[2]

b[2] -> a[2]


a: [0.0, 1.0]
[a[0]=1.0, b[1]=0.0]: 0.000 1.000
[a[0]=0.0, b[1]=1.0]: 0.000 1.000
[a[0]=1.0, b[1]=1.0]: 0.500 0.500
[a[0]=0.0, b[1]=0.0]: 0.500 0.500

b: [0.0, 1.0]
[b[0]=1.0]: 0.750 0.250
[b[0]=0.0]: 1.000 0.000
</code></pre>
<p>Or, outputting to Graphviz...</p>
<pre><code>$ java -jar tDBN-0.1.3.jar -i example3.csv -p 1 -s ll --parameters \
--nonStationary | dot -Tpng -o example3.png</code></pre>
<p>... this graph is produced:</p>
<p><img src="images/example3.png"></p>

            <!--Advanced usage: tDBN-0.1.3.jar uses the class com.github.tDBN.cli.LearnFromFile as entry point. Other entry points exist.-->

         <h3><a name="algorithm"></a>The tDBN learning algorithm</h3>
         <p>
            The tDBN algorithm jointly learns the optimal intra and inter time-slice connectivity of a DBN by constraining the search space to tree augmented networks.  In a tree-augmented network, attributes (nodes) have at most one parent in the same time-slice, hence the intra-slice connectivity is a tree. An attribute can however have several parents from from preceding slices.
         </p>
         <p>
            The following figure illustrates the structure of a tree-augmented network, with 5 attributes and one parent from the previous time-slice:</p>
            <p><img src="images/tree-augmented.png"></p>


<h3>Assessment on time complexity</h3>
        <p>
        The tDBN learning algorithm has a theoretical time complexity of O(n<sup>p+3</sup> r<sup>p+2</sup> N), where n is the number of network attributes, p is the number of parents from the preceding time-slice, r is the number of states of all attributes and N is the number of observations.</p>
        <p>This section presents the results of a set of simulations to assess the algorithm's running time versus its theoretical complexity.  The methodology consisted in 1) generating random tree-augmented networks, 2) sampling each of those networks to generate observations, and 3) inputting the observations to the tDBN algorithm to recover the underlying structure. The time taken by the third step of this process &mdash; the algorithm's running time &mdash; was recorded. Furthermore, the original and recovered networks were compared by evaluating the precision metric.</p>

        <p>Each of the following subsections studies the effect of varying one of the parameters (n, p, r or N), keeping all the others constant. The default values of the parameters are:</p>
        <table>
        <tr><td>n</td><td>10</td></td></tr>
        <tr><td>p</td><td>2</td></td></tr>
        <tr><td>r</td><td>5</td></td></tr>
        <tr><td>N</td><td>100</td></td></tr>
        </table>
        <p>The results are displayed as average statistics over 10 runs, also indicating the standard deviation. All simulations were run on Intel i5-3570 @ 3.40 GHz machines.</p>


        <h4>Number of attributes n</h4>
        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>n</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
5</td><td>0&plusmn;0</td><td>60&plusmn;16</td></tr>
<tr><td>
8</td><td>2&plusmn;0</td><td>38&plusmn;11</td></tr>
<tr><td>
11</td><td>10&plusmn;0</td><td>27&plusmn;7</td></tr>
<tr><td>
14</td><td>31&plusmn;0</td><td>17&plusmn;6</td></tr>
<tr><td>
17</td><td>81&plusmn;1</td><td>14&plusmn;5</td></tr>
<tr><td>
20</td><td>175&plusmn;1</td><td>14&plusmn;4</td></tr>
<tr><td>
23</td><td>326&plusmn;2</td><td>8&plusmn;4</td></tr>
<tr><td>
26</td><td>581&plusmn;2</td><td>9&plusmn;4</td></tr>
<tr><td>
29</td><td>976&plusmn;4</td><td>9&plusmn;6</td></tr>
<tr><td>
32</td><td>1555&plusmn;3</td><td>5&plusmn;2</td></tr>
<tr><td>
35</td><td>2389&plusmn;9</td><td>8&plusmn;2</td></tr>
        </table></p>

        <p><img src="images/n-variation.png"></p>
                <p>The adjustment of a 4-degree polynomial curve to the obtained data (Time vs. n) yields an almost perfect fit, with R&sup2; > 0.9999. This result is in line with the theoretical complexity of O(n<sup>5</sup>), assuming other parameters constant. As n tends to larger values, only a 5-degree polynomial curve is expected to perfectly fit the data.</p>
        <p><table>
        <colgroup><col width="40"/><col width="40"/></colgroup>
        <tr><td>Degree of the fitting polynomial</td><td>R&sup2;</td></tr>
        <tr><td>1</td><td>0.699</td></td></tr>
        <tr><td>2</td><td>0.9636</td></td></tr>
        <tr><td>3</td><td>0.9985</td></td></tr>
        <tr><td>4</td><td>0.99998</td></td></tr>
        <tr><td>5</td><td>0.999997</td></td></tr>
        </table>


        <h4>Number of parents p</h4>
        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>p</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
1</td><td>0&plusmn;0</td><td>69&plusmn;12</td></tr>
<tr><td>
2</td><td>7&plusmn;0</td><td>25&plusmn;8</td></tr>
<tr><td>
3</td><td>79&plusmn;1</td><td>26&plusmn;7</td></tr>
<tr><td>
4</td><td>671&plusmn;18</td><td>36&plusmn;5</td></tr>
<tr><td>
5</td><td>4395*</td><td>44*</td></tr>
        </table>
        <small>* Due to large running time, simulation for p=5 was only run once</small></p>

        <p><img src="images/p-variation.png">&nbsp;<img src="images/p-variation-log.png"></p>

        <table><tr><td>Exponential fit</td><td>R&sup2; = 0.993</td></tr></table>

            <p>The adjustment of an exponential curve to the obtained data (Time vs. p) yields an good fit, with R&sup2; > 0.99. It can be observed in the semi-log graph that the real curve is not a straight line, but has a slight inclination downwards. This suggests that the real complexity is sub-exponential, nevertheless being bounded by O(2<sup>p</sup>), assuming other parameters constant.</p>

        <h4>Size of attributes r</h4>
        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>p</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
2</td><td>0&plusmn;0</td><td>55&plusmn;12</td></tr>
<tr><td>
4</td><td>3&plusmn;0</td><td>37&plusmn;12</td></tr>
<tr><td>
6</td><td>12&plusmn;0</td><td>22&plusmn;9</td></tr>
<tr><td>
8</td><td>33&plusmn;1</td><td>20&plusmn;7</td></tr>
<tr><td>
10</td><td>74&plusmn;1</td><td>22&plusmn;9</td></tr>
<tr><td>
12</td><td>147&plusmn;2</td><td>13&plusmn;7</td></tr>
<tr><td>
14</td><td>259&plusmn;4</td><td>13&plusmn;4</td></tr>
<tr><td>
16</td><td>428&plusmn;3</td><td>17&plusmn;6</td></tr>
<tr><td>
18</td><td>676&plusmn;5</td><td>17&plusmn;5</td></tr>
<tr><td>
20</td><td>1003&plusmn;7</td><td>18&plusmn;8</td></tr>
        </table></p>

        <p><img src="images/r-variation.png"></p>
        <p>The adjustment of a 3-degree polynomial curve to the obtained data (Time vs. r) yields an almost perfect fit, with R&sup2; > 0.9999. This result is in line with the theoretical complexity of O(r<sup>4</sup>), assuming other parameters constant. As n tends to larger values, only a 4-degree polynomial curve is expected to perfectly fit the data.</p>
        <p><table>
        <colgroup><col width="40"/><col width="40"/></colgroup>
        <tr><td>Degree of the fitting polynomial</td><td>R&sup2;</td></tr>
        <tr><td>1</td><td>0.794</td></td></tr>
        <tr><td>2</td><td>0.9882</td></td></tr>
        <tr><td>3</td><td>0.99991</td></td></tr>
        <tr><td>4</td><td>0.99998</td></td></tr>
        </table>

        <h4>Number of observations N</h4>

        <p><table>
        <colgroup><col width="40"/><col width="40"/><col width="40"/></colgroup>
        <tr><td>N</td><td>Time (seconds) </td><td>Precision (%)</td></tr>
        <tr><td>
100</td><td>6&plusmn;0</td><td>32&plusmn;6</td></tr>
<tr><td>
200</td><td>13&plusmn;0</td><td>54&plusmn;16</td></tr>
<tr><td>
300</td><td>20&plusmn;1</td><td>85&plusmn;15</td></tr>
<tr><td>
400</td><td>27&plusmn;1</td><td>97&plusmn;4</td></tr>
<tr><td>
500</td><td>38&plusmn;1</td><td>99&plusmn;1</td></tr>
<tr><td>
600</td><td>47&plusmn;1</td><td>100&plusmn;1</td></tr>
<tr><td>
700</td><td>52&plusmn;2</td><td>99&plusmn;3</td></tr>
<tr><td>
800</td><td>59&plusmn;2</td><td>100&plusmn;1</td></tr>
<tr><td>
900</td><td>70&plusmn;2</td><td>100&plusmn;0</td></tr>
<tr><td>
1000</td><td>77&plusmn;2</td><td>99&plusmn;2</td></tr>
        </table></p>

                <p><img src="images/N-variation.png"></p>

                <p>The adjustment of a linear curve to the obtained data (Time vs. N) yields a good fit, with R&sup2; > 0.99. This result is in line with the theoretical complexity of O(N), assuming other parameters constant.</p>

            <table><tr><td>Linear fit</td><td>R&sup2; = 0.9971</td></tr></table>


         <h3>References</h3>
         <ol>
            <li><a name="ref1"></a>Arbeitman, Michelle N., et al. "Gene expression during the life cycle of Drosophila melanogaster." Science 297.5590 (2002): 2270-2275.</li>
            <li><a name="ref2"></a>Zhao, Wentao, Erchin Serpedin, and Edward R. Dougherty. "Inferring gene regulatory networks from time series data using the minimum description length principle." Bioinformatics 22.17 (2006): 2129-2135.</li>
            <li><a name="ref3"></a>Dondelinger, Frank, Sophie Lèbre, and Dirk Husmeier. "Non-homogeneous dynamic Bayesian networks with Bayesian regularization for inferring gene regulatory networks with gradually time-varying structure." Machine learning 90.2 (2013): 191-230.</li>
         </ol>

      </section>
      <footer>
         <p>This project is maintained by <a href="https://github.com/josemonteiro">josemonteiro</a></p>
         <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
      </div>
   </body>
</html>
